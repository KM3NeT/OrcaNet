# Info for building a model with the ModelBuilder. This will produce a
# compiled keras model, adapted to the input data given to the Organizer.
# This is not required to use orca_train, as any model can be used for that.
#
# The model itself consists of a
# sequence of predefined layer blocks. E.g., the model might consist of convolutional
# layer blocks (convolution, BatchNorm, Activation, MaxPooling and/or Dropout),
# and end with a special last block, e.g. a flatten layer followed by some dense layer blocks,
# which produce a categorical output.
#
# Info for compiling the model is given in the section [compile]. This includes
# the used optimizer (e.g. adam) and the loss functions for each output
# of the network.

# ----------------------------------------------------------------------
[model]
# First, the default values for the layers blocks of the network:
# Type of the block to be added
# See orcanet/builder_util/layer_blocks.py for all available blokcs, as well
# as possible arguments.
type = "conv_block"
# Specifies the dimension of convolutional blocks, either 2 or 3
conv_dim = 3
# Kernel size of the convolution.
kernel_size = 3
# Add a dropout layer with given rate.
dropout = 0.2
# Pool size of a MaxPooling layer, e.g. (1,1,2)
# pool_sizes = 2
# Activation function that should be used. e.g. 'linear', 'relu'...
activation = 'relu'
# Add a batch normalization layer
batchnorm=true
# l2 regularizer for the weights of the layer.
# kernel_l2_reg = 0.00001


# Then, the blocks in the body section of the resulting network.
# Each dict makes a new block. The default values above will be inserted
# only if they are missing, i.e. the values below are not overwritten.
blocks = [
    {filters=64},
    {filters=64},
    {filters=64, pool_size=[2, 2, 2]},
    {filters=64},
    {filters=64},
    {filters=64},
    {filters=128, pool_size=[2, 2, 2]},
    {filters=128},
    {filters=128},
    {filters=128},
    # This is the last layer of the network, in this case its a special output block
    # called OutputRegErr. For each predicted quantity: 3 dense layer blocks +
    # 4 dense layer blocks for respective error
    # Other output blocks are also available.
    {type="OutputRegErr", output_names=['e', 'dx', 'dy', 'dz', 'by', 'vx', 'vy', 'vz', 'vt'], flatten=true}
]

# ----------------------------------------------------------------------
[compile]
# Options for the compilation of the network
# The optimizer to use for compiling
optimizer = "adam"
# Settings for the optimizer
epsilon=0.1

[compile.losses]
# The loss(es) of the model are listed here.
# The keyword is the name of the respective layer in the model, for which
# this loss is used. The value is a dict with a loss function that it will
# use, and optionally a weight to each loss and/or a metric.
dx = {function="mse", weight=10}
dy = {function="mse", weight=10}
dz = {function='mean_absolute_error', weight=15}
vx = {function='mean_absolute_error', weight=1}
vy = {function='mean_absolute_error', weight=1}
vz = {function='mean_absolute_error', weight=1}
vt = {function='mean_absolute_error', weight=1e-10}
e  = {function='mean_absolute_error'}
by = {function='mean_absolute_error', weight=10}

dx_err = {function='loss_uncertainty_mse'}
dy_err = {function='loss_uncertainty_mse'}
dz_err = {function='loss_uncertainty_mse', weight=1.25}
vx_err = {function='loss_uncertainty_mse'}
vy_err = {function='loss_uncertainty_mse'}
vz_err = {function='loss_uncertainty_mse'}
vt_err = {function='loss_uncertainty_mse', weight=1e-10}
e_err  = {function='loss_uncertainty_mse', weight=1e-2}
by_err = {function='loss_uncertainty_mse'}
